---
layout: post
title:  "Deep Q-Networks로 뱀 게임 인공지능 만들기"
date:   2020-02-10 23:59:59
author: choyi0521
tags: [machine-learning, reinforcement-learning, deep-q-learning, keras]
---


## 소개

&nbsp;&nbsp;&nbsp;&nbsp;강화학습 알고리즘을 테스트하기 위해 다양한 라이브러리를 사용할 수 있지만 원하는 환경이 없는 경우가 종종 있습니다. 이런 경우 간단한 환경은 pygame, opencv 같은 라이브러리를 가지고 직접 만들어서 테스트해볼 수 있습니다.

&nbsp;&nbsp;&nbsp;&nbsp;이번 포스트에서는 뱀 게임 환경을 직접 구현하고 강화학습 알고리즘을 적용하는 과정을 살펴볼 것입니다. 이를 위해 pygame으로 뱀 게임을 만들고 Keras로 딥마인드의 "Playing Atari with Deep Reinforcement Learning"에서 소개되었던 DQN(Deep Q-Networks)을 구현 해보겠습니다. 본 포스트에서 다루는 뱀 게임 인공지능 전체 코드는 깃헙(https://github.com/choyi0521/snake-reinforcement-learning)에서 확인할 수 있습니다.

<center>
<img src="/assets/images/snake-dqn/double_feed.gif" width="300" border="1"/>
<br/>
<em>DQN으로 학습한 뱀 게임 인공지능</em>
</center>

## 뱀 게임 규칙

&nbsp;&nbsp;&nbsp;&nbsp;뱀 게임은 1970년대에 처음 등장해서 지금까지 다양한 형태의 변종이 존재합니다. 이번 프로젝트에서는 강화학습의 효과를 쉽게 확인하기 위해서 간단한 형태의 뱀 게임 환경을 구현하겠습니다. 우리의 뱀 게임은 다음과 같은 규칙을 따라 진행됩니다.

* $m \times n$ 크기의 필드가 주어집니다. 각 칸은 비어 있거나 장애물, 먹이, 혹은 뱀의 일부로 이루어져 있습니다. 뱀은 하나의 실 형태로 존재합니다.
* 뱀은 일정 시간마다 앞, 왼쪽, 오른쪽 방향 중 하나를 선택해 움직입니다. 이때, 몸통 부분은 고정되어 있고 머리 부분이 늘어나고 꼬리 부분이 줄어듭니다.
* 뱀이 장애물 혹은 자신의 몸통에 부딪히거나 필드 밖으로 나가면 게임이 종료됩니다.
* 뱀이 먹이에 닿으면 길이가 증가합니다. 이때, 먹이는 빈 칸 중 하나에 무작위로 생성됩니다.

&nbsp;&nbsp;&nbsp;&nbsp;게임의 목표는 뱀을 최대한 길어지게 만드는 것입니다. 이를 위해서 뱀이 자신의 몸통과 장애물을 피해가면서 먹이를 많이 먹을 수 있도록 이동하는 전략이 필요할 것입니다.


## 뱀 게임 디자인
### 블록

&nbsp;&nbsp;&nbsp;&nbsp;블록의 종류로는 빈 칸, 장애물, 먹이, 뱀의 머리, 뱀의 몸통, 뱀의 꼬리가 있습니다. 빈 칸, 장애물, 먹이 블록은 각각 하나의 형태만 존재하지만 뱀의 머리, 몸통, 꼬리 블록은 각각 방향에 따라 다양한 형태를 가지고 있습니다.

<style>  
table {border-collapse:collapse;}
th, td {border:1px solid black; text-align:center;}
td {min-width: 30px; max-width: 30px; overflow: hidden;}
</style>
<table>
	<tr>
		<th>종류</th>
		<td><font size=1>빈 칸</font></td>
		<td><font size=1>방해물</font></td>
		<td><font size=1>먹이</font></td>
		<td colspan='4'><font size=1>뱀 머리</font></td>
		<td colspan='6'><font size=1>뱀 몸통</font></td>
		<td colspan='4'><font size=1>뱀 꼬리</font></td>
	</tr>
	<tr>
		<th>번호</th>
        <td>0</td>
        <td>1</td>
        <td>2</td>
        <td>3</td>
        <td>4</td>
        <td>5</td>
        <td>6</td>
        <td>7</td>
        <td>8</td>
        <td>9</td>
        <td>10</td>
        <td>11</td>
        <td>12</td>
        <td>13</td>
        <td>14</td>
        <td>15</td>
        <td>16</td>
    </tr>
	<tr>
		<th>형태</th>
        <td><img src="/assets/images/snake-dqn/blocks/0.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/1.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/2.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/3.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/4.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/5.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/6.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/7.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/8.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/9.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/10.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/11.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/12.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/13.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/14.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/15.png"/></td>
        <td><img src="/assets/images/snake-dqn/blocks/16.png"/></td>
	</tr>
</table>

&nbsp;&nbsp;&nbsp;&nbsp;각 블록은 번호, 색깔, 폴리곤을 이루는 점들 정보를 가지고 있습니다. 번호는 필드를 뉴럴넷 입력 값으로 임베딩할 때 원-핫 인코딩하는 데에 사용되고 색깔과 점들 정보는 랜더링할 때 사용됩니다.
```python
class Block:
    @staticmethod
    def contains(**args):
        pass

    @staticmethod
    def get_code(**args):
        pass

    @staticmethod
    def get_color(**args):
        pass

    @staticmethod
    def get_points(**args):
        pass
```
&nbsp;&nbsp;&nbsp;&nbsp;실제 구현에서는 블록 종류별로 위의 block을 상속받아서 클래스를 만들고 각 함수를 따로 구현했습니다. 폴리곤 모양을 하드 코딩해서 구현했기 때문에 코드가 상당히 깁니다. 자세한 내용은 깃헙 코드를 참고해 주세요.

### 뱀

&nbsp;&nbsp;&nbsp;&nbsp;아래 그림은 필드의 좌표계와 방향의 번호를 나타냅니다. 뱀은 꼬리의 위치와 꼬리에서 시작해서 뱀의 머리가 뻗어나간 방향의 수열로 나타낼 수 있습니다. 예를 들어 그림에 있는 뱀은 꼬리 위치 (4, 1)과 방향 수열 [0, 0, 1, 1, 0, 1, 1, 2, 2, 1, 2, 1, 1, 0, 3]로 나타낼 수 있습니다.


<center>
<img src="/assets/images/snake-dqn/coordinate.png" width="500"/>
</center>

&nbsp;&nbsp;&nbsp;&nbsp;방향 수열에서 첫 번째 수는 꼬리의 방향을 나타내고 마지막 수는 머리의 방향을 나타냅니다. 또한, 수열에서 각 연속한 방향 쌍으로 뱀의 몸통을 나타낼 수 있습니다.

&nbsp;&nbsp;&nbsp;&nbsp;위의 뱀을 예시로 들어보겠습니다. 방향 수열의 첫 번째 수는 0으로 꼬리의 연결부가 위를 향하게 됩니다. 그리고 방향 수열의 마지막 수는 3으로 머리가 왼쪽을 향하게 됩니다. 방향 수열에서 연속된 모든 쌍은 (0, 0), (0, 1), (1, 1), ..., (1, 1), (1, 0), (0, 3)이고 이들은 각각 뱀 몸통인 11, 9, 12, ..., 7, 12, 8번 블록과 대응됩니다. 아래 그림은 필드의 각 블록에 해당하는 번호를 나타냅니다.

<center>
<img src="/assets/images/snake-dqn/field.png" width="400"/>
</center>


## 강화학습 개요

&nbsp;&nbsp;&nbsp;&nbsp;강화학습의 목표는 에이전트가 환경을 탐색하면서 보상을 최대화할 수 있는 정책을 찾는 것이라고 할 수 있습니다. 매 시점 $t$에 에이전트는 자신의 상태 $S_t$와 가능한 행동 집합 $A(S_t)$를 가지고 있습니다. 에이전트는 행동 $A_t \in A(S_t)$를 수행하고 환경으로부터 다음 상태인 $S_{t+1}$과 보상 $R_{t+1}$을 받습니다. 에이전트는 환경과의 상호작용으로부터 누적 보상값을 최대화할 수 있는 정책을 찾게 됩니다. 이번 절에서는 뱀 게임 인공지능 구현체에서 강화학습의 각 요소를 찾아 설명해보겠습니다.

<center>
<img src="/assets/images/snake-dqn/reinforcement_learning_cycle.png" width="400"/>
</center>

### 행동

&nbsp;&nbsp;&nbsp;&nbsp;일정 시간마다 에이전트는 전진(MOVE_FORWARD), 좌회전 후 전진(TURN_LEFT), 우회전 후 전진(TURN_RIGHT) 총 세가지 중 하나의 행동을 선택해야 합니다. 각 행동의 번호는 다음과 같이 정의되어 있습니다.

```python
class SnakeAction:
    MOVE_FORWARD = 0
    TURN_LEFT = 1
    TURN_RIGHT = 2
```

### 보상

&nbsp;&nbsp;&nbsp;&nbsp;에이전트는 행동을 선택할 때마다 보상을 받게 됩니다. 선택한 행동에 의해 게임이 종료되었다면 -1, 먹이를 먹지 않고 이동을 했다면 0의 보상을 얻습니다. 뱀의 길이가 증가할수록 먹이를 얻기 힘들어지는 점을 고려하여 먹이를 먹은 경우 최종 뱀의 길이를 보상으로 얻도록 만들었습니다.

### 상태

&nbsp;&nbsp;&nbsp;&nbsp;강화학습을 하기 위해서는 환경에서 발생되는 모든 정보를 알 수 있도록 상태를 정의하는 것이 좋습니다. 놀랍게도 우리가 정의한 뱀 게임은 필드 정보만을 가지고도 모든 정보를 알 수 있습니다. 다시 말해, $i$번째 필드를 $S_i$라고 할 때 다음이 성립합니다.

$$P(S_{t+1}|S_t) = P(S_{t+1}|S_1, ..., S_t)$$

&nbsp;&nbsp;&nbsp;&nbsp;참고로 우리가 단 하나의 블록을 사용해서 뱀을 나타냈다면 위 식이 성립하지 않습니다. 초록색 블럭으로 뱀을 나타냈을 때 $i$번째 필드를 $S'_i$라고 정의하겠습니다. 다음과 같은 상황을 생각해보세요.

<center>
<img src="/assets/images/snake-dqn/s1s2.png" width="400"/>
</center>

&nbsp;&nbsp;&nbsp;&nbsp;$S'_1, S'_2$ 모두를 관찰했다면 (2, 1)에 뱀의 머리가 있다(어느 쪽을 향하고 있는지는 알 수 없습니다)는 사실을 알 수 있지만 $S'_2$만 관찰하면 네 블록 중 어느 블록에 뱀의 머리가 있는지 알 수 없습니다. 따라서 $P(S'_3|S'_2) \neq P(S'_3|S'_1, S'_2)$가 됩니다. 이 문제는 뱀의 머리와 나머지 부분을 따로 구분하는 것만으로는 해결되지 않습니다. 필드를 통해 모든 정보를 얻을 수 있도록 하려면 필드 안에 뱀의 모양과 진행 방향 등의 정보가 모두 포함되어 있어야 합니다.

&nbsp;&nbsp;&nbsp;&nbsp;뱀의 모양과 관련된 정보를 필드 안에 포함하지 않고 따로 피쳐를 만드는 것을 고려할 수도 있습니다. 하지만, 이 경우에는 다른 두 유형의 피쳐를 학습할 수 있도록 복잡한 뉴럴넷을 설계해야 하므로 간단히 강화학습을 테스트하자는 취지와 맞지 않는다고 보았습니다. 여러가지를 고려한 결과, 필드 정보만 상태로 정의해도 괜찮도록 이전 절에서 소개한 방식으로 뱀을 디자인했습니다.


### 전이

&nbsp;&nbsp;&nbsp;&nbsp;에이전트가 선택할 수 있는 총 세 가지의 행동(MOVE_FORWARD, TURN_LEFT, TURN_RIGHT)에 따라 상태를 적절히 변화시켜야 합니다. 이를 쉽게 구현하기 위해서 필드 정보 이외에도 뱀 머리의 위치, 뱀 꼬리의 위치, 방향 수열, 뱀의 현재 진행 방향을 변수로 저장하였습니다.

&nbsp;&nbsp;&nbsp;&nbsp;TURN_LEFT를 수행하는 것은 뱀의 현재 진행 방향을 왼쪽으로 변경하고 MOVE_FORWARD를 수행하는 것과 같습니다. 마찬가지로 TURN_RIGHT는 현재 진행 방향을 오른쪽으로 변경하고 MOVE_FORWARD를 수행하는 것과 같을 것입니다. 따라서 MOVE_FORWARD에 따른 상태 변화를 구현하면 나머지 두 행동에 따른 생태 변화는 앞에 방향 전환을 추가하는 식으로 쉽게 구현할 수 있습니다.

&nbsp;&nbsp;&nbsp;&nbsp;MOVE_FORWARD를 수행해서 뱀이 이동할 때 뱀의 모든 부분을 다시 수정할 필요는 없습니다. 움직이는 형태를 보면 뱀의 앞부분과 뒷부분에만 변화가 있고 뱀의 중간 부분은 변하지 않기 때문입니다. 뱀의 머리와 꼬리 모양은 뱀의 방향 수열을 업데이트했을 때 바뀌는 부분에 맞춰 수정해주면 됩니다. 다음과 같이 몇 가지 경우를 고려해서 상태를 변화시킵니다.

1. 뱀 머리의 진행 방향에 벽, 장애물, 뱀 몸통이 있는 경우 게임을 종료합니다. 뱀의 꼬리가 있는 경우는 상관없다는 점에 주의합시다.
2. 뱀 머리의 진행 방향에 먹이가 없는 경우 뱀의 머리와 꼬리를 옮깁니다.
3. 뱀 머리의 진행 방향에 먹이가 있는 경우 뱀의 머리만 옮기고 새로운 먹이를 생성합니다.

&nbsp;&nbsp;&nbsp;&nbsp;상태와 전이를 다루는 부분은 SnakeStateTransition 클래스에서 확인할 수 있습니다. get_state 함수는 상태를 리턴하는 함수로 필드를 원-핫 인코딩하여 제공합니다. 따라서 상태의 크기는 (높이)$\times$(너비)$\times$17이 됩니다. move_forward, turn_left, turn_right는 전이 관련 함수로 상태변화에 따른 보상과 게임 종료 여부를 리턴합니다.

```python
class SnakeStateTransition:
    DX, DY = [-1, 0, 1, 0], [0, 1, 0, -1]

    def __init__(self, field_size, field, num_feed, initial_head_position, initial_tail_position, initial_snake):
        self.field_height, self.field_width = field_size
        self.field = field.copy()
        self.hx, self.hy = initial_head_position
        self.tx, self.ty = initial_tail_position
        self.snake = deque(initial_snake)
        self.direction = initial_snake[-1]

        for _ in range(num_feed):
            self._generate_feed()

    def _generate_feed(self):
        empty_blocks = []
        for i in range(self.field_height):
            for j in range(self.field_width):
                if self.field[i][j] == EmptyBlock.get_code():
                    empty_blocks.append((i, j))

        if len(empty_blocks) > 0:
            x, y = random.sample(empty_blocks, 1)[0]
            self.field[x, y] = FeedBlock.get_code()

    def get_state(self):
        return np.eye(NUM_CHANNELS)[self.field]

    def get_length(self):
        return len(self.snake) + 1

    def move_forward(self):
        hx = self.hx + SnakeStateTransition.DX[self.direction]
        hy = self.hy + SnakeStateTransition.DY[self.direction]
        if hx < 0 or hx >= self.field_height or hy < 0 or hy >= self.field_width \
                or ObstacleBlock.contains(self.field[hx][hy]) \
                or SnakeBodyBlock.contains(self.field[hx][hy]):
            return -1, True

        is_feed = FeedBlock.contains(self.field[hx][hy])

        if not is_feed:
            self.field[self.tx, self.ty] = EmptyBlock.get_code()
            td = self.snake.popleft()
            self.tx += SnakeStateTransition.DX[td]
            self.ty += SnakeStateTransition.DY[td]
            self.field[self.tx, self.ty] = SnakeTailBlock.get_code(self.snake[0])

        self.snake.append(self.direction)
        self.field[self.hx, self.hy] = SnakeBodyBlock.get_code(self.snake[-1], self.snake[-2])
        self.field[hx, hy] = SnakeHeadBlock.get_code(self.snake[-1])
        self.hx, self.hy = hx, hy

        if is_feed:
            self._generate_feed()
            return self.get_length(), False

        return 0, False

    def turn_left(self):
        self.direction = (self.direction + 3) % 4
        return self.move_forward()

    def turn_right(self):
        self.direction = (self.direction + 1) % 4
        return self.move_forward()
```

### 환경

&nbsp;&nbsp;&nbsp;&nbsp;환경 클래스는 OpenAI의 gym 라이브러리 스타일로 구현했습니다. reset 함수는 초기 상태를 리턴하고, step 함수는 행동을 입력받고 변화된 상태, 보상, 게임 종료 여부를 리턴합니다. render 함수는 현재 상태를 이미지로 만들어 화면에 출력하는 역할을 합니다. 필드를 출력하기 위해 필드의 각 블록 번호를 가지고 색깔, 점들 위치를 가져오고 pygame의 함수를 이용하여 폴리곤을 그립니다.

```python
class Snake:
    ACTIONS = {
        SnakeAction.MOVE_FORWARD: 'move_forward',
        SnakeAction.TURN_LEFT: 'turn_left',
        SnakeAction.TURN_RIGHT: 'turn_right'
    }

    def __init__(self, level_loader, block_pixels=30):
        self.level_loader = level_loader
        self.block_pixels = block_pixels

        self.field_height, self.field_width = self.level_loader.get_field_size()

        pygame.init()
        self.screen = pygame.display.set_mode((
            self.field_width * block_pixels,
            self.field_height * block_pixels
        ))
        self.clock = pygame.time.Clock()

        self.reset()

    def reset(self):
        self.state_transition = SnakeStateTransition(
            self.level_loader.get_field_size(),
            self.level_loader.get_field(),
            self.level_loader.get_num_feed(),
            self.level_loader.get_initial_head_position(),
            self.level_loader.get_initial_tail_position(),
            self.level_loader.get_initial_snake()
        )
        self.tot_reward = 0
        return self.state_transition.get_state()

    def step(self, action):
        reward, done = getattr(self.state_transition, Snake.ACTIONS[action])()
        self.tot_reward += reward
        return self.state_transition.get_state(), reward, done

    def get_length(self):
        return self.state_transition.get_length()

    def quit(self):
        pygame.quit()

    def render(self, fps):
        pygame.display.set_caption('length: {}'.format(self.state_transition.get_length()))
        pygame.event.pump()
        self.screen.fill((255, 255, 255))

        for i in range(self.field_height):
            for j in range(self.field_width):
                cp = get_color_points(self.state_transition.field[i][j])
                if cp is None:
                    continue
                pygame.draw.polygon(
                    self.screen,
                    cp[0],
                    (cp[1] + [j, i])*self.block_pixels
                )

        pygame.display.flip()
        self.clock.tick(fps)

    def save_image(self, save_path):
        pygame.image.save(self.screen, save_path)
```

### 에이전트

&nbsp;&nbsp;&nbsp;&nbsp;에이전트는 상태와 보상을 확인하고 정책에 기반하여 행동을 선택합니다. 우리가 정의한 상태는 필드 정보인데 이를 가지고 최적의 행동을 어떤 것인지 평가하기 위해서는 복잡한 함수가 필요해 보입니다. 이 문제를 해결하기 위해 고차원의 입력값으로부터 정책을 학습하는 강화학습의 한 방법인 DQN(deep q-networks)을 사용할 것입니다.

## Deep Q-Networks

### Replay Memory

### Q-Network

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 7, 7, 32)          4928
_________________________________________________________________
dropout_1 (Dropout)          (None, 7, 7, 32)          0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 5, 5, 32)          9248
_________________________________________________________________
dropout_2 (Dropout)          (None, 5, 5, 32)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 800)               0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               205056
_________________________________________________________________
dropout_3 (Dropout)          (None, 256)               0
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 771
=================================================================
Total params: 220,003
Trainable params: 220,003
Non-trainable params: 0
_________________________________________________________________
```


## 학습